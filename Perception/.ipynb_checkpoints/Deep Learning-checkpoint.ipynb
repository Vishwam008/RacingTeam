{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d195df6",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "Uses algorithms inspired by the structure and function of the brain's neural networks.\n",
    "\n",
    "\n",
    "## Artificial Neural Networks\n",
    "Artificial neural networks are a type of deep learning model that is based on the structure of brain's neural networks.\n",
    "\n",
    "Consist of connected units called neurons and  each neuron receives, processes and passes a signal to lower neurons.\n",
    "\n",
    "Neurons are organized in layers-- input, hidden and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5553999",
   "metadata": {},
   "source": [
    "### Keras Sequential model\n",
    "Sequential is a linear stack of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf315502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0a8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(5, input_shape=(3,), activation='relu'), \n",
    "    # 5 is the number of nodes and input_shape is the shape of the input to the layer\n",
    "    # First layer of the network requires the input shape\n",
    "    # 'relu' is the activation function\n",
    "    Dense(2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a57f78d",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "<ul>\n",
    "    <li>Deep or fully connected</li>\n",
    "    <li>Convolutional</li>\n",
    "    <li>Pooling</li>\n",
    "    <li>Recurrent</li>\n",
    "    <li>Normalization</li>\n",
    "</ul>\n",
    "\n",
    "### Deeply connected layer\n",
    "In a deeply connected layer, all nodes of the previous layer are connected to all of the current layer. \n",
    "\n",
    "Connections imply data tranfer between the nodes. The final input to the activation function of a node is a weighted sum of all the inputs received.\n",
    "\n",
    "The weight of an individual input is a number between 0 and 1. These weights are optimized by the model to get the best output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a01be",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "A function that operates on the input of any node and produces an output.\n",
    "\n",
    "<b>Relu function: </b>Returns 0 if the input <= 0 and the input itself otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d0ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method of defining a model is:\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_shape=(3,)))\n",
    "model.add(Activation('relu')) # adding an activation layer separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb7ba1",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The output obtained is due to the weights used in the deep learning model.\n",
    "\n",
    "During training the model is supplied with both input and output. The weights are arbitrarily chosen at first and the model predicts an output according to this.\n",
    "\n",
    "The loss function is then calculated as the difference between the actual ouput and the predicted output or any other function such as the MSE etc.\n",
    "\n",
    "New weight w1 = w1 - [d(loss)/d(w1)]  *  (Learning rate)\n",
    "\n",
    "Learning rate is generally between 0.001 and 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "435033b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d18c1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, input_shape=(1,), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d89a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Adam is an optimizer that optimizes the weights, loss function(up) and the metric used for judging the predicted output is the accuracy of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21533304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sparse_categorical_crossentropy'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(scaled_train_samples, train_labels, batch_size=10, epochs=20, shuffle=True, verbose=2)\n",
    "# input data, given output\n",
    "# batch size is the number of inputs given at a time\n",
    "# epoch- number of times the given data is reiterated\n",
    "# shuffle- the data is randomized wrt order\n",
    "# verbose is the level of the output:\n",
    "#     0-silent, 1-progress bar, 2-report of individual epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d01be3c",
   "metadata": {},
   "source": [
    " ## Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cb4a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b7f7d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6363dec",
   "metadata": {},
   "source": [
    "## Validation set\n",
    "We split the given dataset into 3 sets which are training, validation and testing.\n",
    "\n",
    "Validation set is a labelled set that is used on the model after each epoch to check its accuracy.\n",
    "\n",
    "Test is an unlabelled dataset used at the end of training(and validation) to test the accuracy of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(scaled_train_samples, train_labels, validation_split=0.20, batch_size=10, epochs=20, shuffle=True, verbose=2)\n",
    "# 20% of the training set is set aside as validation set\n",
    "# or use validation_set=<set> instead of val_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37bafe",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_samples, batch_size=10, verbose=0)\n",
    "for i in predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ae7c7",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "The model predicts data from training set really well but is not able to predict data from test set.\n",
    "\n",
    "Ways to eliminate this are:\n",
    "<ul>\n",
    "    <li>Adding more data</li>\n",
    "    <li>Data augmentation: Cropping, flipping, rotating images</li>\n",
    "    <li>Reducing complexity of the model</li>\n",
    "    <li>Dropout: dropping some of the nodes</li>\n",
    "</ul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70d2d9",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "Model is not able to predict data of the training set\n",
    "\n",
    "Ways to eliminate:\n",
    "<ul>\n",
    "    <li>Increasing the complexity</li>\n",
    "    <li>Decreasing the dropout rate (If accuracy is high in the validation set but not in the training set.)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857a9cf",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "The data is given along with their respective labels\n",
    "\n",
    "Example: Model to predict gender based on height and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "234483d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(2,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a6a0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "956d0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = [[150, 67], [130, 60], [200, 65], [125, 52], [230, 72], [181, 70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8617590",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [1, 1, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e39c43bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 - 1s - loss: 13.0168 - accuracy: 0.5000 - 504ms/epoch - 252ms/step\n",
      "Epoch 2/10\n",
      "2/2 - 0s - loss: 12.7970 - accuracy: 0.5000 - 9ms/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "2/2 - 0s - loss: 12.5044 - accuracy: 0.5000 - 9ms/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "2/2 - 0s - loss: 12.2127 - accuracy: 0.5000 - 7ms/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "2/2 - 0s - loss: 12.0175 - accuracy: 0.5000 - 7ms/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "2/2 - 0s - loss: 11.8126 - accuracy: 0.5000 - 8ms/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "2/2 - 0s - loss: 11.5276 - accuracy: 0.5000 - 7ms/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "2/2 - 0s - loss: 11.2765 - accuracy: 0.5000 - 6ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "2/2 - 0s - loss: 11.0395 - accuracy: 0.5000 - 6ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "2/2 - 0s - loss: 10.8307 - accuracy: 0.5000 - 8ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13165a6b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_samples, y=train_labels, batch_size=3, epochs=10, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3559603",
   "metadata": {},
   "source": [
    "### Semi Supervised learning\n",
    "Suppose we have a dataset that is only partially labelled.\n",
    "\n",
    "We take the labelled data and train our model on it.\n",
    "\n",
    "We then label the unlabelled data (called pseudolabelling) and now we have a whole dataset to train our model upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2c597",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "Encodes categorical data into integers or vectors.\n",
    "\n",
    "Length of vectors is equal to the dimension of the sample space of categories. \n",
    "\n",
    "A column of the same index in every vector corresponds to a certain category and in every vector only of the entries can be 1 and the rest are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb8438",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "Have convolutional hidden layers. \n",
    "\n",
    "A convolution involves applying one or more filters on a given image. \n",
    "\n",
    "\n",
    "Majority of concepts discussed are already covered..\n",
    "\n",
    "### Zero Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd0d8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import *\n",
    "from keras.layers.core import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05f8fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_valid = Sequential([\n",
    "    Dense(16, activation='relu', input_shape= (20, 20, 3)), \n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', padding='valid'), # valid is default\n",
    "    Conv2D (64, kernel_size=(5, 5), activation='relu', padding='valid'), \n",
    "    Conv2D(128, kernel_size=(7, 7), activation='relu', padding='valid'),\n",
    "    Flatten(), \n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "599fdfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 20, 20, 16)        64        \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 18, 18, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 8, 8, 128)         401536    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 2)                 16386     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 473,890\n",
      "Trainable params: 473,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_valid.summary() # at the end output becomes 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddf9423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_same = Sequential([\n",
    "    Dense(16, activation='relu', input_shape= (20, 20, 3)), \n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'), \n",
    "    Conv2D (64, kernel_size=(5, 5), activation='relu', padding='same'), \n",
    "    Conv2D(128, kernel_size=(7, 7), activation='relu', padding='same'),\n",
    "    Flatten(), \n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cd25bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_17 (Dense)            (None, 20, 20, 16)        64        \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 20, 20, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 20, 20, 64)        51264     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 20, 20, 128)       401536    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 51200)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 2)                 102402    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 559,906\n",
      "Trainable params: 559,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_same.summary() # shape is maintained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e00934",
   "metadata": {},
   "source": [
    "## Max Pooling\n",
    "It reduces the dimensions of the image.\n",
    "\n",
    "This is done by applying a filter that returns the maximum pixel value among the pixels that it is currently operating on.\n",
    "\n",
    "In addition to this we also define a stride value that determines the pixels by which the filter moves after an operation.\n",
    "\n",
    "<b>Uses: </b>For scaling down and thus reducing the computational load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27f00af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.pooling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "770017b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential ([\n",
    "    Dense(16, activation='relu', input_shape= (20,20, 3)),\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'), \n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'),\n",
    "    Conv2D(64, kernel_size=(5, 5), activation='relu', padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1220603",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "For calculating the derivative of the loss function wrt a certain weight, we need to have a function relating the weight to the loss function.\n",
    "\n",
    "Backpropagation is the process by which we obtain this relation.\n",
    "\n",
    "#### What if the gradient is vanishing?\n",
    "If the gradient is really small then the corresponding weight will eb changed by a really small value. Thus the weight will not change considerably and will never reach close to its optimal value even after many epochs.\n",
    "\n",
    "#### Gradient explosion\n",
    "If the gradient is really high then the weight will changeby a very big value after each epoch and the likelihood of attaining an optimal value is less.\n",
    "\n",
    "### Weight Initialization\n",
    "##### Xavier Initialization\n",
    "If we initialize the weights randomly then their variance is n [number of nodes in the previous layer] and S.D. is sqrt(n). \n",
    "\n",
    "Now if we pass on these weights to the activation function then it is very likely to take on a value greater than 1. If using the sigmoid function, the derivative at such points>1 is vanishing.\n",
    "\n",
    "Thus we need to bring the variance close to 1/n [statistical data]. This is done by multiplying the randomized weights by 1/sqrt(n) for sigmoid and 2/sqrt(n) for relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8858273",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential ([\n",
    "    Dense(16, input_shape=(1,5), activation='relu'),\n",
    "    Dense (32, activation='relu', kernel_initializer='glorot_uniform'), # this is default glorot_normal can also be used\n",
    "    Dense (2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41610d89",
   "metadata": {},
   "source": [
    "### Bias\n",
    "It is similar to a threshold value that determines if the given neuron will fire or not\n",
    "\n",
    "Biases are learnable.\n",
    "\n",
    "For relu the bias is 0 and any value below it is rejected(0). To change the bias, we need to subtract the bias from the weighted average of inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063568c",
   "metadata": {},
   "source": [
    "## Learnable Parameters\n",
    "The values which are changed during training of a model.\n",
    "\n",
    "Include the weights and biases.\n",
    "\n",
    "For any layer in a model the number of learnable parameters are given by (input x output + biases) \n",
    "\n",
    "For a dense layer, input is the number of inputs from the previous layer and the output is the number of nodes.\n",
    "\n",
    "For a convolutional layer, the input is the # of filters in prev, output is the # x size of curr filters and bias is the # of curr filters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ef12d",
   "metadata": {},
   "source": [
    "### L2 Regularization\n",
    "We add a term x in the loss function that penalizes for large weights. This reduces the complexity in our model and reduces overfitting.\n",
    "\n",
    "x is sum of the squared norms of the weight matrices multiplied by l/2m\n",
    "\n",
    "l is the regularization param. and m is the number of inputs.\n",
    "\n",
    "Main objective is that the loss function will be so high and the weights so low that the effect of some layers might be cancelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b56e576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1150cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(2,)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)), # 0.01 is the reg. param.\n",
    "    Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac06a4b",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "No. of data passed at a time. Higher batch size allows faster training time and generalization of model but a very high would result into computer overloading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b19427",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "Standardization: x = (x - mean)/s.d.\n",
    "\n",
    "This is to reduce the range of the given data\n",
    "\n",
    "causes exploding gradient problem as the ranges of all data serie are not the same or even similar.\n",
    "\n",
    "Batch norm. applies this to each layer in the model which eliminates the outlying weights. This is done in the following way:\n",
    "\n",
    "x = (x - mean)/s.d.\n",
    "\n",
    "and then x*g + b\n",
    "\n",
    "where g and b are arbitrary constants and learnable param. Thus the mean and s.d. of the batch is varied and thus they too become learnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92e17aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f37e7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(2,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(axis=1),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
